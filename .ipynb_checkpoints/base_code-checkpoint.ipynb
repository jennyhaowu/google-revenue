{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import psutil\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "PATH=\"../data/\"\n",
    "NUM_ROUNDS = 20000\n",
    "VERBOSE_EVAL = 500\n",
    "STOP_ROUNDS = 100\n",
    "N_SPLITS = 10\n",
    "\n",
    " #the columns that will be parsed to extract the fields from the jsons\n",
    "cols_to_parse = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "\n",
    "def read_parse_dataframe(file_name):\n",
    "    #full path for the data file\n",
    "    path = PATH + file_name\n",
    "    #read the data file, convert the columns in the list of columns to parse using json loader,\n",
    "    #convert the `fullVisitorId` field as a string\n",
    "    data_df = pd.read_csv(path, \n",
    "        converters={column: json.loads for column in cols_to_parse}, \n",
    "        dtype={'fullVisitorId': 'str'})\n",
    "    #parse the json-type columns\n",
    "    for col in cols_to_parse:\n",
    "        #each column became a dataset, with the columns the fields of the Json type object\n",
    "        json_col_df = json_normalize(data_df[col])\n",
    "        json_col_df.columns = [f\"{col}_{sub_col}\" for sub_col in json_col_df.columns]\n",
    "        #we drop the object column processed and we add the columns created from the json fields\n",
    "        data_df = data_df.drop(col, axis=1).merge(json_col_df, right_index=True, left_index=True)\n",
    "    return data_df\n",
    "    \n",
    "def process_date_time(data_df):\n",
    "    print(\"process date time ...\")\n",
    "    data_df['date'] = data_df['date'].astype(str)\n",
    "    data_df[\"date\"] = data_df[\"date\"].apply(lambda x : x[:4] + \"-\" + x[4:6] + \"-\" + x[6:])\n",
    "    data_df[\"date\"] = pd.to_datetime(data_df[\"date\"])   \n",
    "    data_df[\"year\"] = data_df['date'].dt.year\n",
    "    data_df[\"month\"] = data_df['date'].dt.month\n",
    "    data_df[\"day\"] = data_df['date'].dt.day\n",
    "    data_df[\"weekday\"] = data_df['date'].dt.weekday\n",
    "    data_df['weekofyear'] = data_df['date'].dt.weekofyear\n",
    "    data_df['month_unique_user_count'] = data_df.groupby('month')['fullVisitorId'].transform('nunique')\n",
    "    data_df['day_unique_user_count'] = data_df.groupby('day')['fullVisitorId'].transform('nunique')\n",
    "    data_df['weekday_unique_user_count'] = data_df.groupby('weekday')['fullVisitorId'].transform('nunique')\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def process_format(data_df):\n",
    "    print(\"process format ...\")\n",
    "    for col in ['visitNumber', 'totals_hits', 'totals_pageviews']:\n",
    "        data_df[col] = data_df[col].astype(float)\n",
    "    data_df['trafficSource_adwordsClickInfo.isVideoAd'].fillna(True, inplace=True)\n",
    "    data_df['trafficSource_isTrueDirect'].fillna(False, inplace=True)\n",
    "    return data_df\n",
    "    \n",
    "def process_device(data_df):\n",
    "    print(\"process device ...\")\n",
    "    data_df['browser_category'] = data_df['device_browser'] + '_' + data_df['device_deviceCategory']\n",
    "    data_df['browser_os'] = data_df['device_browser'] + '_' + data_df['device_operatingSystem']\n",
    "    return data_df\n",
    "\n",
    "def process_totals(data_df):\n",
    "    print(\"process totals ...\")\n",
    "    data_df['visitNumber'] = np.log1p(data_df['visitNumber'])\n",
    "    data_df['totals_hits'] = np.log1p(data_df['totals_hits'])\n",
    "    data_df['totals_pageviews'] = np.log1p(data_df['totals_pageviews'].fillna(0))\n",
    "    data_df['mean_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('mean')\n",
    "#    data_df['median_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('median')\n",
    "    data_df['sum_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('sum')\n",
    "    data_df['max_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('max')\n",
    "    data_df['min_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('min')\n",
    "    data_df['var_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('var')\n",
    "    data_df['mean_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('mean')\n",
    "#    data_df['median_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('median')\n",
    "    data_df['sum_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('sum')\n",
    "    data_df['max_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('max')\n",
    "    data_df['min_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('min')    \n",
    "    return data_df\n",
    "\n",
    "def process_geo_network(data_df):\n",
    "    print(\"process geo network ...\")\n",
    "    data_df['sum_pageviews_per_network_domain'] = data_df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('sum')\n",
    "    data_df['count_pageviews_per_network_domain'] = data_df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('count')\n",
    "    data_df['mean_pageviews_per_network_domain'] = data_df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('mean')\n",
    "    data_df['sum_hits_per_network_domain'] = data_df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('sum')\n",
    "    data_df['count_hits_per_network_domain'] = data_df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('count')\n",
    "    data_df['mean_hits_per_network_domain'] = data_df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('mean')\n",
    "    return data_df\n",
    "\n",
    "def process_traffic_source(data_df):\n",
    "    print(\"process traffic source ...\")\n",
    "    data_df['source_country'] = data_df['trafficSource_source'] + '_' + data_df['geoNetwork_country']\n",
    "    data_df['campaign_medium'] = data_df['trafficSource_campaign'] + '_' + data_df['trafficSource_medium']\n",
    "    data_df['medium_hits_mean'] = data_df.groupby(['trafficSource_medium'])['totals_hits'].transform('mean')\n",
    "#    data_df['medium_hits_median'] = data_df.groupby(['trafficSource_medium'])['totals_hits'].transform('median')\n",
    "    data_df['medium_hits_max'] = data_df.groupby(['trafficSource_medium'])['totals_hits'].transform('max')\n",
    "    data_df['medium_hits_min'] = data_df.groupby(['trafficSource_medium'])['totals_hits'].transform('min')\n",
    "    data_df['medium_hits_sum'] = data_df.groupby(['trafficSource_medium'])['totals_hits'].transform('sum')\n",
    "    return data_df\n",
    "\n",
    "#Feature processing\n",
    "## Load data\n",
    "print('reading train')\n",
    "train_df = read_parse_dataframe('train.csv')\n",
    "trn_len = train_df.shape[0]\n",
    "train_df = process_date_time(train_df)\n",
    "print('reading test')\n",
    "test_df = read_parse_dataframe('test.csv')\n",
    "test_df = process_date_time(test_df)\n",
    "\n",
    "## Drop columns\n",
    "cols_to_drop = [col for col in train_df.columns if train_df[col].nunique(dropna=False) == 1]\n",
    "train_df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "test_df.drop([col for col in cols_to_drop if col in test_df.columns], axis=1, inplace=True)\n",
    "\n",
    "###only one not null value\n",
    "train_df.drop(['trafficSource_campaignCode'], axis=1, inplace=True)\n",
    "\n",
    "###converting columns format\n",
    "train_df['totals_transactionRevenue'] = train_df['totals_transactionRevenue'].astype(float)\n",
    "train_df['totals_transactionRevenue'] = train_df['totals_transactionRevenue'].fillna(0)\n",
    "train_df['totals_transactionRevenue'] = np.log1p(train_df['totals_transactionRevenue'])\n",
    "\n",
    "\n",
    "## Features engineering\n",
    "train_df = process_format(train_df)\n",
    "train_df = process_device(train_df)\n",
    "train_df = process_totals(train_df)\n",
    "train_df = process_geo_network(train_df)\n",
    "train_df = process_traffic_source(train_df)\n",
    "\n",
    "test_df = process_format(test_df)\n",
    "test_df = process_device(test_df)\n",
    "test_df = process_totals(test_df)\n",
    "test_df = process_geo_network(test_df)\n",
    "test_df = process_traffic_source(test_df)\n",
    "\n",
    "## Categorical columns\n",
    "print(\"process categorical columns ...\")\n",
    "num_cols = ['month_unique_user_count', 'day_unique_user_count', 'weekday_unique_user_count',\n",
    "            'visitNumber', 'totals_hits', 'totals_pageviews', \n",
    "            'mean_hits_per_day', 'sum_hits_per_day', 'min_hits_per_day', 'max_hits_per_day', 'var_hits_per_day',\n",
    "            'mean_pageviews_per_day', 'sum_pageviews_per_day', 'min_pageviews_per_day', 'max_pageviews_per_day',\n",
    "            'sum_pageviews_per_network_domain', 'count_pageviews_per_network_domain', 'mean_pageviews_per_network_domain',\n",
    "            'sum_hits_per_network_domain', 'count_hits_per_network_domain', 'mean_hits_per_network_domain',\n",
    "            'medium_hits_mean','medium_hits_min','medium_hits_max','medium_hits_sum',\n",
    "            'median_hits_per_day', 'median_pageviews_per_day', 'medium_hits_median']\n",
    "            \n",
    "not_used_cols = [\"visitNumber\", \"date\", \"fullVisitorId\", \"sessionId\", \n",
    "        \"visitId\", \"visitStartTime\", 'totals_transactionRevenue', 'trafficSource_referralPath']\n",
    "cat_cols = [col for col in train_df.columns if col not in num_cols and col not in not_used_cols]\n",
    "\n",
    "merged_df = pd.concat([train_df, test_df])\n",
    "print('Cat columns : ', len(cat_cols))\n",
    "ohe_cols = []\n",
    "for i in cat_cols:\n",
    "    if len(set(merged_df[i].values)) < 100:\n",
    "        ohe_cols.append(i)\n",
    "\n",
    "print('ohe_cols : ', ohe_cols)\n",
    "print(len(ohe_cols))\n",
    "merged_df = pd.get_dummies(merged_df, columns = ohe_cols)\n",
    "train_df = merged_df[:trn_len]\n",
    "test_df = merged_df[trn_len:]\n",
    "del merged_df\n",
    "gc.collect()\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in ohe_cols:\n",
    "        continue\n",
    "    #print(col)\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n",
    "    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n",
    "    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))\n",
    "\n",
    "print('FINAL train shape : ', train_df.shape, ' test shape : ', test_df.shape)\n",
    "#print(train_df.columns)\n",
    "train_df = train_df.sort_values('date')\n",
    "X = train_df.drop(not_used_cols, axis=1)\n",
    "y = train_df['totals_transactionRevenue']\n",
    "X_test = test_df.drop([col for col in not_used_cols if col in test_df.columns], axis=1)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "             \n",
    "lgb_params1 = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n",
    "               \"max_depth\": 8, \"min_child_samples\": 20, \n",
    "               \"reg_alpha\": 1, \"reg_lambda\": 1,\n",
    "               \"num_leaves\" : 257, \"learning_rate\" : 0.01, \n",
    "               \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \n",
    "               \"verbosity\": -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_target(x):\n",
    "    if x < 2:\n",
    "        return 0\n",
    "    elif x < 4:\n",
    "        return 1\n",
    "    elif x < 6:\n",
    "        return 2\n",
    "    elif x < 8:\n",
    "        return 3\n",
    "    elif x < 10:\n",
    "        return 4\n",
    "    elif x < 12:\n",
    "        return 5\n",
    "    elif x < 14:\n",
    "        return 6\n",
    "    elif x < 16:\n",
    "        return 7\n",
    "    elif x < 18:\n",
    "        return 8\n",
    "    elif x < 20:\n",
    "        return 9\n",
    "    elif x < 22:\n",
    "        return 10\n",
    "    else:\n",
    "        return 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_categorized = y.apply(categorize_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "def kfold_lgb_xgb():\n",
    "    FOLDs = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "\n",
    "    oof_lgb = np.zeros(len(train_df))\n",
    "    predictions_lgb = np.zeros(len(test_df))\n",
    "\n",
    "    features_lgb = list(X.columns)\n",
    "    feature_importance_df_lgb = pd.DataFrame()\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(X, y_categorized)):\n",
    "        trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n",
    "        val_data = lgb.Dataset(X.iloc[val_idx], label=y.iloc[val_idx])\n",
    "\n",
    "        print(\"LGB \" + str(fold_) + \"-\" * 50)\n",
    "        num_round = 20000\n",
    "        clf = lgb.train(lgb_params1, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 100)\n",
    "        oof_lgb[val_idx] = clf.predict(X.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "\n",
    "        fold_importance_df_lgb = pd.DataFrame()\n",
    "        fold_importance_df_lgb[\"feature\"] = features_lgb\n",
    "        fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df_lgb[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n",
    "        predictions_lgb += clf.predict(X_test, num_iteration=clf.best_iteration) / FOLDs.n_splits\n",
    "\n",
    "    #lgb.plot_importance(clf, max_num_features=30)    \n",
    "    cols = feature_importance_df_lgb[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:50].index\n",
    "    best_features_lgb = feature_importance_df_lgb.loc[feature_importance_df_lgb.feature.isin(cols)]\n",
    "    plt.figure(figsize=(14,10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features_lgb.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances.png')\n",
    "    x = []\n",
    "    for i in oof_lgb:\n",
    "        if i < 0:\n",
    "            x.append(0.0)\n",
    "        else:\n",
    "            x.append(i)\n",
    "    cv_lgb = mean_squared_error(x, y)**0.5\n",
    "    cv_lgb = str(cv_lgb)\n",
    "    cv_lgb = cv_lgb[:10]\n",
    "\n",
    "    pd.DataFrame({'preds': x}).to_csv('lgb_oof_' + cv_lgb + '.csv', index = False)\n",
    "\n",
    "    print(\"CV_LGB : \", cv_lgb)\n",
    "    return cv_lgb, predictions_lgb\n",
    "\n",
    "cv_lgb, lgb_ans = kfold_lgb_xgb()\n",
    "x = []\n",
    "for i in lgb_ans:\n",
    "    if i < 0:\n",
    "        x.append(0.0)\n",
    "    else:\n",
    "        x.append(i)\n",
    "np.save('lgb_ans.npy', x)\n",
    "submission = test_df[['fullVisitorId']].copy()\n",
    "submission.loc[:, 'PredictedLogRevenue'] = x\n",
    "submission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"].apply(lambda x : 0.0 if x < 0 else x)\n",
    "submission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"].fillna(0.0)\n",
    "grouped_test = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\n",
    "grouped_test.to_csv('lgb_' + cv_lgb + '.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
